1. lr模型是线性模型还是非线性，为什么？能推导它的原理吗？
Logistic Regression (LR) 模型实际上是一种线性模型，考虑二分类问题，LR 模型的目标是预测一个样本属于某一类别的概率。LR 模型使用 sigmoid 函数将线性组合映射到 [0, 1] 的区间，从而得到概率值。因此，虽然 LR 模型通过 sigmoid 函数引入了非线性映射，但整体上仍然是一个线性模型。

2. 常用的防止过拟合的技术手段有哪些？
（1）正则化（Regularization）：
	（a）L1 正则化（L1 Regularization）： 在损失函数中添加 L1 范数的惩罚项，通过最小化权重向量中的绝对值之和来防止过拟合。L1 正则化可以促使模型参数变得稀疏，有助于特征选择。

	（b）L2 正则化（L2 Regularization）： 在损失函数中添加 L2 范数的惩罚项，通过最小化权重向量的平方和来防止过拟合。L2 正则化有助于权重的平滑分布。

（2）数据扩增（Data Augmentation）：
	通过对训练数据进行一系列的随机变换（如旋转、平移、翻转等），生成新的训练样本，增加数据的多样性，减少过拟合的风险。

（3）早停（Early Stopping）：
	监控模型在验证集上的性能，当性能不再提升时停止训练，以避免过度拟合训练数据。

（4）Dropout：
	在训练过程中，随机将一部分神经元置零，以减少神经网络的复杂性，降低过拟合风险。
（5）集成学习（Ensemble Learning）：
	将多个模型的预测结果进行组合，例如通过投票或取平均值，以提高模型的泛化能力。

3. l1-norm和l2-norm的区别是什么？
（1）L1-norm：
	（a）L1-norm 是指向量中各个元素的绝对值之和。
	（b）在正则化中，L1 正则化通过添加权重向量的 L1-norm 作为惩罚项来限制模型的复杂性。
	（c）L1 正则化有助于稀疏化权重向量，促使模型对于不重要的特征赋予较小的权重，因此可以用于特征选择。

（2）L2-norm：
	（a）L2-norm 是指向量中各个元素的平方和的平方根。
	（b）在正则化中，L2 正则化通过添加权重向量的 L2-norm 的平方作为惩罚项来限制模型的复杂性。
	（c）L2 正则化有助于平滑化权重向量，防止某个特征对模型的影响过于显著，有助于防止过拟合。

综上所述，L1-norm 和 L2-norm 在正则化中的应用有不同的效果，L1 正则化倾向于产生稀疏权重，而 L2 正则化倾向于产生平滑权重。在实际应用中，它们也可以同时使用，这就是所谓的 Elastic Net 正则化。

4. 请详细的解释self-attention机制？他和attention机制有什么不同？
Self-attention机制是一种注意力机制的变体，用于处理序列数据，例如自然语言文本。它与传统的attention机制不同之处在于，self-attention允许模型在同一个序列中不同位置的元素之间建立不同的关联性，而传统的attention机制通常是在两个不同序列之间建立关联。

与传统的attention机制不同，self-attention中的Query、Key和Value来自同一个输入序列，而传统attention中的Query和Key通常来自不同的序列。self-attention因此更加灵活，能够处理更多种类的关联关系。

从公式上来看，他们的计算公式是一样的，但是QK的来源不同。

5. 为什么batch normalization？

释义：早期的解释主要是基于概率分布的，大概意思是将每一层的输入分布都归一化到N(0,1)上，减少了所谓的Internal Covariate Shift，从而稳定乃至加速了训练。《How Does Batch Normalization Help Optimization?》里边，作者明确地提出了上述质疑，否定了原来的一些观点，并提出了自己关于BN的新理解：他们认为BN主要作用是使得整个损失函数的landscape更为平滑，从而使得我们可以更平稳地进行训练。


BZ缺点：1、不适合RNN。序列长度不可知，但BZ需要某一层的所有输出。计算时很麻烦。2、依赖mini batch大小，在大批量时候效果好，不适合在线学习。

与layer normaliztion区别：1、做法上，BZ需要对某一层的输出做归一化，而LN是对某一个样本的输出在各个维度上的值做归一化，不需要。2、实践证明，LN用于RNN进行Normalization时，取得了比BN更好的效果。但用于CNN时，效果并不如BN明显。

推理时只有一个样本怎么做BZ？可以用从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量。

6、文本相似度计算方式？
（1）embedding + cosine相似度计算
（2）ngram
（3）TF-IDF
（4）训练一个模型出来，实际上就是个文本分类的任务。

7. 词向量举例：

word2vector：核心思想是通过词的上下文得到词的向量化表示，有两种方法：CBOW（通过附近词预测中心词）、Skip-gram（通过中心词预测附近的词）。

glove：word2vec只考虑到了词的局部信息，glove利用共现矩阵设计损失函数，同时考虑了局部信息和整体的信息。

8. BERT-base或BERT-large的区别是什么？
（1）BERT-base：包含12个Transformer Encoder层，每层有12个注意力头。总共有110M个参数。
（2）BERT-large：包含24个Transformer Encoder层，每层有16个注意力头。总共有340M个参数。





















